{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Functions\n",
    "\n",
    "Here I set up the functions we will use to parse the files.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function for writing output\n",
    "def write_file(df, fileout):\n",
    "    gilded = df[df['gilded'] > 0]\n",
    "    print 'Writing', gilded.shape[0], 'entries to', fileout\n",
    "    if os.path.isfile(fileout):\n",
    "        gilded.to_csv(fileout, mode='a', header=False, encoding='utf-8', index=False)\n",
    "    else:\n",
    "        gilded.to_csv(fileout, encoding='utf-8', index=False)\n",
    "\n",
    "# Function for reading file with given parameters\n",
    "def parse_file(filein, amount, fileout):\n",
    "    print 'Opening', filein\n",
    "    with open(filein, 'r') as f:\n",
    "        itt = 1 # Line counter\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line[:-1]))\n",
    "            itt+=1\n",
    "            if itt > amount: # If chunk_size is reached, write to csv and clear memory\n",
    "                if lines:\n",
    "                    write_file(pd.DataFrame(lines), fileout)\n",
    "                lines = [] # Clear list (free memory)\n",
    "                itt = 1 # Reset line counter\n",
    "        if lines: # Write final lines of file (if any)\n",
    "            write_file(pd.DataFrame(lines), fileout)\n",
    "            lines = []\n",
    "    print 'Complete! All gilded records from', filein, 'written to', fileout            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Data\n",
    "\n",
    "### Inputs:\n",
    "\n",
    "**filepath_in**<br>\n",
    "*Directory to read the input file from*\n",
    "\n",
    "**input_filename**<br> \n",
    "*Name of the input file*\n",
    "\n",
    "**filepath_out**<br>\n",
    "*Directory to write the output file*\n",
    "\n",
    "**output_filename**<br> \n",
    "*Name of the output file*\n",
    "\n",
    "**chunk_size**<br>\n",
    "*How many lines to read before processing and writing*\n",
    "* *too few and you'll have too many write operations*\n",
    "* *too many and you'll use too much memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath_in = ''\n",
    "input_filename = ''\n",
    "filepath_out = ''\n",
    "output_filename = ''\n",
    "chunk_size = 100000\n",
    "\n",
    "# For single file use\n",
    "parse_file(filepath_in + input_filename, chunk_size, filepath_out + output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Revised Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv \n",
    "\n",
    "# Function for writing output\n",
    "def write_top10_file(df, fileout):\n",
    "    top10 = ['AskReddit', 'pics', 'funny', 'videos', 'todayilearned', 'AdviceAnimals', 'news', 'WTF', 'worldnews', 'nfl']\n",
    "    t = df[(df['subreddit'].isin(top10)) & (df['parent_id'].str.startswith('t3_', na=False))]\n",
    "    print 'Writing', t.shape[0], 'entries.'\n",
    "    for subreddit in top10:\n",
    "        if os.path.isfile(fileout + '_' + subreddit + '.csv'):\n",
    "            t[t['subreddit'] == subreddit].to_csv(fileout + '_' + subreddit + '.csv', mode='a', header=False, encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        else:\n",
    "            t[t['subreddit'] == subreddit].to_csv(fileout + '_' + subreddit + '.csv', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# Function for reading file with given parameters\n",
    "def parse_file_for_top10(filein, amount, fileout):\n",
    "    cols = ['archived','author','author_flair_css_class','author_flair_text','body','controversiality','created_utc','distinguished','downs','edited','gilded','id','link_id','name','parent_id','removal_reason','retrieved_on','score','score_hidden','subreddit','subreddit_id','ups']\n",
    "    print 'Opening', filein\n",
    "    with open(filein, 'r') as f:\n",
    "        itt = 1 # Line counter\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line[:-1]))\n",
    "            itt+=1\n",
    "            if itt > amount: # If chunk_size is reached, write to csv and clear memory\n",
    "                if lines:\n",
    "                    write_top10_file(pd.DataFrame(lines, columns=cols), fileout)\n",
    "                lines = [] # Clear list (free memory)\n",
    "                itt = 1 # Reset line counter\n",
    "        if lines: # Write final lines of file (if any)\n",
    "            write_top10_file(pd.DataFrame(lines, columns=cols), fileout)\n",
    "        lines = []\n",
    "    print 'Complete! All top10 subreddit records from', filein, 'written to', fileout + '_[subreddit].csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening C:/Users/jsmoo/Desktop/Reddit Data/2015_reddit_comments_corpus/reddit_data/2014/RC_2014-12\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/jsmoo/Desktop/Reddit Data/2015_reddit_comments_corpus/reddit_data/2014/RC_2014-12'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8e59011c5b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m# For single file use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mparse_file_for_top10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_in\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath_out\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutput_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-830cfac50764>\u001b[0m in \u001b[0;36mparse_file_for_top10\u001b[0;34m(filein, amount, fileout)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'archived'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author_flair_css_class'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'author_flair_text'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'controversiality'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'created_utc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'distinguished'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'downs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'edited'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'gilded'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'link_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'parent_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'removal_reason'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'retrieved_on'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'score_hidden'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'subreddit_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ups'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'Opening'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilein\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilein\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mitt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# Line counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/jsmoo/Desktop/Reddit Data/2015_reddit_comments_corpus/reddit_data/2014/RC_2014-12'"
     ]
    }
   ],
   "source": [
    "filepath_in = 'C:/Users/jsmoo/Desktop/Reddit Data/2015_reddit_comments_corpus/reddit_data/2015/'\n",
    "input_filename = 'RC_2015-01'\n",
    "filepath_out = 'C:/Users/jsmoo/Desktop/01/'\n",
    "output_filename = 'RC_2015-01_toplevel'\n",
    "chunk_size = 400000\n",
    "\n",
    "# For single file use\n",
    "parse_file_for_top10(filepath_in + input_filename, chunk_size, filepath_out + output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| subreddit     | gilded | ungilded |  total  |\n",
    "|---------------|:------:|:--------:|:-------:|\n",
    "| AdviceAnimals |   552  |  807547  |  808099 |\n",
    "| AskReddit     |  6194  |  9017540 | 9023734 |\n",
    "| funny         |   910  |  1483540 | 1484450 |\n",
    "| news          |   524  |  501371  |  501895 |\n",
    "| nfl           |   454  |  877144  |  877598 |\n",
    "| pics          |   955  |  1264050 | 1265005 |\n",
    "| todayilearned |   475  |  637739  |  638214 |\n",
    "| videos        |   695  |  876532  |  877227 |\n",
    "| worldnews     |   443  |  540900  |  541343 |\n",
    "| WTF           |   490  |  725334  |  725824 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6194 9017540 9023734\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "relavant = ['author', 'author_flair_css_class', 'author_flair_text', 'body', 'created_utc', 'distinguished', 'edited', 'gilded', 'score', 'subreddit']\n",
    "\n",
    "subreddit = 'AskReddit'\n",
    "\n",
    "data = pd.read_csv('C:/Users/jsmoo/Desktop/2015/05/RC_2015-05_toplevel_' + subreddit + '.csv', usecols=relavant)\n",
    "data = data.append(pd.read_csv('C:/Users/jsmoo/Desktop/2015/04/RC_2015-04_toplevel_' + subreddit + '.csv', usecols=relavant), ignore_index=True)\n",
    "data = data.append(pd.read_csv('C:/Users/jsmoo/Desktop/2015/03/RC_2015-03_toplevel_' + subreddit + '.csv', usecols=relavant), ignore_index=True)\n",
    "data = data.append(pd.read_csv('C:/Users/jsmoo/Desktop/2015/02/RC_2015-02_toplevel_' + subreddit + '.csv', usecols=relavant), ignore_index=True)\n",
    "data = data.append(pd.read_csv('C:/Users/jsmoo/Desktop/2015/01/RC_2015-01_toplevel_' + subreddit + '.csv', usecols=relavant), ignore_index=True)\n",
    "\n",
    "print data[data['gilded'] > 0].shape[0], data[data['gilded'] == 0].shape[0], data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "\n",
    "# Sample 400 of each type\n",
    "gilded = data[data['gilded'] > 0].sample(400) #.sort('gilded', ascending=False)[:400]\n",
    "ungilded = data[data['gilded'] == 0].sample(400) #.sort('ups', ascending=False)[:400]\n",
    "\n",
    "# Pull 200 from each and put into train/test sets\n",
    "train = gilded[:200].append(ungilded[:200], ignore_index=True)\n",
    "test = gilded[200:].append(ungilded[200:], ignore_index=True)\n",
    "\n",
    "shuffle(train).to_csv('W:/CSCE489/Start Data/Train/' + subreddit + '_train.csv', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "shuffle(test).to_csv('W:/CSCE489/Start Data/Test/' + subreddit + '_test.csv', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = None"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs489]",
   "language": "python",
   "name": "conda-env-cs489-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
