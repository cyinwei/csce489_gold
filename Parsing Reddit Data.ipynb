{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Functions\n",
    "\n",
    "Here I set up the functions we will use to parse the files.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function for writing output\n",
    "def write_file(df, fileout):\n",
    "    gilded = df[df['gilded'] > 0]\n",
    "    print 'Writing', gilded.shape[0], 'entries to', fileout\n",
    "    if os.path.isfile(fileout):\n",
    "        gilded.to_csv(fileout, mode='a', header=False, encoding='utf-8', index=False)\n",
    "    else:\n",
    "        gilded.to_csv(fileout, encoding='utf-8', index=False)\n",
    "\n",
    "# Function for reading file with given parameters\n",
    "def parse_file(filein, amount, fileout):\n",
    "    print 'Opening', filein\n",
    "    with open(filein, 'r') as f:\n",
    "        itt = 1 # Line counter\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line[:-1]))\n",
    "            itt+=1\n",
    "            if itt > amount: # If chunk_size is reached, write to csv and clear memory\n",
    "                if lines:\n",
    "                    write_file(pd.DataFrame(lines), fileout)\n",
    "                lines = [] # Clear list (free memory)\n",
    "                itt = 1 # Reset line counter\n",
    "        if lines: # Write final lines of file (if any)\n",
    "            write_file(pd.DataFrame(lines), fileout)\n",
    "            lines = []\n",
    "    print 'Complete! All gilded records from', filein, 'written to', fileout            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Data\n",
    "\n",
    "### Inputs:\n",
    "\n",
    "**filepath_in**<br>\n",
    "*Directory to read the input file from*\n",
    "\n",
    "**input_filename**<br> \n",
    "*Name of the input file*\n",
    "\n",
    "**filepath_out**<br>\n",
    "*Directory to write the output file*\n",
    "\n",
    "**output_filename**<br> \n",
    "*Name of the output file*\n",
    "\n",
    "**chunk_size**<br>\n",
    "*How many lines to read before processing and writing*\n",
    "* *too few and you'll have too many write operations*\n",
    "* *too many and you'll use too much memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath_in = ''\n",
    "input_filename = ''\n",
    "filepath_out = ''\n",
    "output_filename = ''\n",
    "chunk_size = 100000\n",
    "\n",
    "# For single file use\n",
    "parse_file(filepath_in + input_filename, chunk_size, filepath_out + output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Revised Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv \n",
    "\n",
    "# Function for writing output\n",
    "def write_top10_file(df, fileout):\n",
    "    top10 = ['AskReddit', 'pics', 'funny', 'videos', 'todayilearned', 'AdviceAnimals', 'news', 'WTF', 'worldnews', 'nfl']\n",
    "    t = df[(df['subreddit'].isin(top10)) & (df['parent_id'].str.startswith('t3_', na=False))]\n",
    "    print 'Writing', t.shape[0], 'entries.'\n",
    "    for subreddit in top10:\n",
    "        if os.path.isfile(fileout + '_' + subreddit + '.csv'):\n",
    "            t[t['subreddit'] == subreddit].to_csv(fileout + '_' + subreddit + '.csv', mode='a', header=False, encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        else:\n",
    "            t[t['subreddit'] == subreddit].to_csv(fileout + '_' + subreddit + '.csv', encoding='utf-8', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# Function for reading file with given parameters\n",
    "def parse_file_for_top10(filein, amount, fileout):\n",
    "    print 'Opening', filein\n",
    "    with open(filein, 'r') as f:\n",
    "        itt = 1 # Line counter\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line[:-1]))\n",
    "            itt+=1\n",
    "            if itt > amount: # If chunk_size is reached, write to csv and clear memory\n",
    "                if lines:\n",
    "                    write_top10_file(pd.DataFrame(lines), fileout)\n",
    "                lines = [] # Clear list (free memory)\n",
    "                itt = 1 # Reset line counter\n",
    "        if lines: # Write final lines of file (if any)\n",
    "            write_top10_file(pd.DataFrame(lines), fileout)\n",
    "        lines = []\n",
    "    print 'Complete! All top10 subreddit records from', filein, 'written to', fileout + '_[subreddit].csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath_in = 'C:/Users/jsmoo/Desktop/Reddit Data/2015_reddit_comments_corpus/reddit_data/2015/'\n",
    "input_filename = 'RC_2015-01'\n",
    "filepath_out = 'W:/CSCE489/Start Data/test/'\n",
    "output_filename = 'RC_2015-01_toplevel'\n",
    "chunk_size = 300000\n",
    "\n",
    "# For single file use\n",
    "parse_file_for_top10(filepath_in + input_filename, chunk_size, filepath_out + output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gilded: int64\n",
      "score: int64\n",
      "ups: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('W:/CSCE489/Start Data/test/RC_2015-01_toplevel_AdviceAnimals.csv')\n",
    "\n",
    "# check1 = data['gilded'].apply(lambda x: str(x).isdigit())\n",
    "# check2 = data['score'].apply(lambda x: str(x).isdigit())\n",
    "# check3 = data['ups'].apply(lambda x: str(x).isdigit())\n",
    "\n",
    "# data = data[(check1 == True)]\n",
    "\n",
    "print 'gilded:', data['gilded'].dtype\n",
    "# print data['gilded'].unique()\n",
    "print 'score:', data['score'].dtype\n",
    "# print data['score'].unique()\n",
    "print 'ups:', data['ups'].dtype\n",
    "# print data['ups'].unique()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs489]",
   "language": "python",
   "name": "conda-env-cs489-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
